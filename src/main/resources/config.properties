#spark job 运行模式
spark.deploy.mode=local
#spark job 名称
spark.application.name=persona
#处理后的用户行为埋点数据，在HDFS的位置
point.file.path=/home/luck/share/spark-job-persona/persona.json
#读取HDFS的partition数量
#指定数据的分区，如果不指定分区，当你的核数大于2的时候，不指定分区数那么就是 2
hdfs.minpartitions=2